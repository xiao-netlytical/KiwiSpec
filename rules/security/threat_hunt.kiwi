
define read_path=../sample_data/zeek/conn.json, 
read_dns_path=../sample_data/zeek/dns.json, 
write_path=../sample_data/result

#Discovery-Network Service Scanning 
read read_path as flows
CREATE [{"source": source, "dport":dport, "dhost": dhost}] as r
VAR i SELECT
    flows[i]["id.orig_h"] as source;
    flows[i]["id.orig_h"] as source;
    COLLECT SET(flows[i]["id.resp_h"]) as  dhost;
    COLLECT SET(flows[i]["id.resp_p"]) as  dport;
    where flows[i]["local_orig" | True] and flows[i]["local_resp" | True];    
    flows[i]["conn_state"] in ["S0", "REJ"]
write write_path/top_sender.json from r

#Discovery-Network Service Scanning, sort by dport for r1 and dhost for r2, and limited to 10 
read read_path as flows
CREATE [{"source": source, "dport":dport, "dport_c":dport_c}] AS r1 ;
       [{"source": source, "dhost": dhost, "dhost_c": dhost_c}] AS r2  
VAR i SELECT
    flows[i]["id.orig_h"] as source;
    COLLECT SET(flows[i]["id.resp_h"]) as dhost;
    COLLECT SET(flows[i]["id.resp_p"]) as dport;
    COUNT DISTINCT(flows[i]["id.resp_h"]) as dhost_c;
    COUNT DISTINCT(flows[i]["id.resp_p"]) as dport_c;
    where flows[i]["local_orig" | True] and flows[i]["local_resp" | True];    
    flows[i]["conn_state"] in ["S0", "REJ"];
    ORDER BY dhost_c DESC LIMIT 10 FOR r2;  dport_c DESC LIMIT 10 FOR r1
write write_path/top_sender_dport.json from r1, 
	  write_path/top_sender_dhost.json from r2

#longest connection, or concatenated connections limit 100
READ read_path AS flows, 
write_path/ip_to_servers.json as srv_names
CREATE [{"src":s_ip, "dst":d_ip, "srv":proto, "flows": ct, "duration":dur}] AS r
VAR i SELECT
    flows[i]["id.orig_h"] AS s_ip;
    flows[i]["id.resp_h"] AS d_ip;
    flows[i]["proto"].upper()+"_"+str(flows[i]["id.resp_p"]) AS proto;
    count distinct(i) AS ct;
    SUM(flows[i]["duration" | 0]) AS dur;
    ORDER BY dur DESC LIMIT 100
update r 
var i set
    r[i]["server"] as srv_names[r[i]["src"] | []]
WRITE write_path/top_duration.json from r

#Extrafiltration, get top sending bytes in total and top sending count in tatal between two paire, the resule should be sorted and limited to top 10
read read_path as flows, write_path/ip_to_servers.json as srv_names
CREATE [{"src":s_ip, "dst":d_ip, "orig_pkts": orig_pkts}] AS r1 ;
       [{"src":s_ip, "dst":d_ip, "orig_bytes": orig_bytes}] AS r2  
VAR i SELECT
    flows[i]["id.orig_h"] AS s_ip;
    flows[i]["id.resp_h"] AS d_ip;
    sum(flows[i]["orig_pkts" | 0]) as orig_pkts;
    sum(flows[i]["orig_bytes" | 0]) as orig_bytes;
    ORDER BY orig_pkts DESC LIMIT 10 FOR r1;  orig_bytes DESC LIMIT 10 FOR r2
update var i set
    r1[i]["server"] as srv_names[r1[i]["src"] | []]

update var i set
    r2[i]["server"] as srv_names[r2[i]["src"] | []]

write write_path/top_bytes_sender.json from r2, 
	  write_path/top_pkts_sender.json from r1

# Beacons - Get connections start with regular intervals
read read_path as flows
create [{"src":s_ip, "dst":d_ip, "intervals": window_ct}] as r0
var i  select
    flows[i]["id.orig_h"] AS s_ip;
    flows[i]["id.resp_h"] AS d_ip;
    int(epoch_time(flows[i]["ts"])) as window_start;
    count distinct(window_start) group by s_ip,d_ip as window_ct;
    order by window_ct DESC LIMIT 100
write write_path/top_consistent_interval.json from r

# DNS - As channel
read read_dns_path as flows, 
write_path/ip_to_servers.json as srv_names
create [{"src":s_ip, "base_domain":domain, "requests": ct}] as r
var i  select 
    flows[i]["id.orig_h"] as s_ip;
    ".".join(flows[i]["query"].split(".")[-2:]) as domain;
    count distinct(i) GROUP BY s_ip,domain AS ct;
    order by ct DESC
update r var i set
    r[i]["server"] as srv_names[r[i]["src"] | []]
 write write_path/dns_domain_requests.json from r   

read write_path/top_duration.json as top_dur_ip, write_path/top_bytes_sender.json as top_byte_ip
create {s_ip_1: srv} as r
var i, j select
    top_byte_ip[i]["src"] as s_ip_1;
    top_byte_ip[i]["server"] as srv;
    top_dur_ip[j]["src"] as s_ip_2;
where s_ip_1 == s_ip_2
WRITE write_path/top_duration_and_out_byte.json from r

