
define read_path_flow=../sample_data/zeek/conn.json, 
read_dns_path=../sample_data/zeek/dns.json, 
write_path=../sample_data/result

#########################################
#DNS request for each source address in a five minute window ordered by request count
read read_path_flow as flows
create {window_start: [{dns_src: dns_requests} LIMITED 10]}  as r
var i  select
    flows[i]["id.orig_h"] as dns_src;
    int(epoch_time(flows[i]["ts"])/300) as window_start;
    count distinct(i) as dns_requests;
    where flows[i]["id.resp_p"] == 53;
    order by dns_requests desc
write write_path/top_dns_sender.json from r

####################
#every five minute interval, get DNS request count for per window and per source address
read read_path_flow as flows
create {window_start: {dns_src: cnt}}  as r
var i  select
    flows[i]["id.orig_h"] as dns_src;
    int(epoch_time(flows[i]["ts"])/300) as window_start;
    count distinct(i) group by dns_src, window_start as cnt;
where flows[i]["id.resp_p"] == 53
write write_path/5_min_dns_request.json from r

####################
# aggregate the connections between a source and destination pair, 
# and sort the source and destination pairs based on the distinct protocols
read read_path_flow as flows
create [{"ip":s_ip, "pkts": pkts, "bytes":bytes, "relationship":relationship}] as r
var i select
    flows[i]["id.orig_h"] as s_ip;
    count distinct(flows[i]["proto"].upper()+"_"+str(flows[i]["id.resp_p"])) as relationship;
    sum(flows[i]["orig_pkts" | 0] + flows[i]["resp_pkts" | 0]) as pkts;
    sum(flows[i]["orig_bytes" | 0] + flows[i]["resp_bytes" | 0]) as bytes;
order by relationship desc
write write_path/top_out_rel.json from r

####################
# path recording of the SMB connections

READ read_path_flow AS flows; write_path/ip_to_servers.json AS srv
create   [path_recording] as result
var i, j select
    flows[i]["id.orig_h"] AS s_ip_1;
    flows[i]["id.resp_h"] AS d_ip_1;
    flows[j]["id.orig_h"] AS s_ip_2;
    flows[j]["id.resp_h"] AS d_ip_2;
    collect set ((s_ip_1, d_ip_1)) where d_ip_1 == s_ip_2 extend by (s_ip_2, d_ip_2) as path_recording;
where flows[i]["id.resp_p"] in [445, 137, 138,139]; flows[j]["id.resp_p"] in [445, 137, 138,139]
WRITE write_path/path_recording.json FROM result

##################
#Discovery-Network Service Scanning: sort by dport for r1 and dhost for r2, and limited to 10,
#for failed or rejected connections with internal traffic only

read read_path_flow as flows
CREATE [{"source": source, "dport":dport, "dport_c":dport_c}] AS r1 ;
       [{"source": source, "dhost": dhost, "dhost_c": dhost_c}] AS r2  
VAR i SELECT
    flows[i]["id.orig_h"] as source;
    COLLECT SET(flows[i]["id.resp_h"]) as dhost;
    COLLECT SET(flows[i]["id.resp_p"]) as dport;
    COUNT DISTINCT(flows[i]["id.resp_h"]) as dhost_c;
    COUNT DISTINCT(flows[i]["id.resp_p"]) as dport_c;
    where flows[i]["local_orig" | True] and flows[i]["local_resp" | True];    
    flows[i]["conn_state"] in ["S0", "REJ"];
    ORDER BY dhost_c DESC LIMIT 10 FOR r2;  dport_c DESC LIMIT 10 FOR r1

UPDATE VAR i set
r1[i]["dport"] AS ",".join(str(r1[i]["dport"]))
write write_path/top_sender_dport.json from r1; 
	  write_path/top_sender_dhost.json from r2

##################
#longest connection, or concatenated connections limit 100, the source server name is updated
READ read_path_flow AS flows; 
write_path/ip_to_servers.json as srv_names
CREATE [{"src":s_ip, "dst":d_ip, "srv":proto, "flows": ct, "duration":dur}] AS r
VAR i SELECT
    flows[i]["id.orig_h"] AS s_ip;
    flows[i]["id.resp_h"] AS d_ip;
    flows[i]["proto"].upper()+"_"+str(flows[i]["id.resp_p"]) AS proto;
    count distinct(i) AS ct;
    SUM(flows[i]["duration" | 0]) AS dur;
    ORDER BY dur DESC LIMIT 100
update r 
var i set
    r[i]["server"] as srv_names[r[i]["src"] | []]
WRITE write_path/top_duration.json from r

##################
#Extrafiltration, get top sending bytes in total and top sending count in tatal between two paire, the resule should be sorted and limited to top 10
read read_path_flow as flows; write_path/ip_to_servers.json as srv_names
CREATE [{"src":s_ip, "dst":d_ip, "orig_pkts": orig_pkts}] AS r1 ;
       [{"src":s_ip, "dst":d_ip, "orig_bytes": orig_bytes}] AS r2  
VAR i SELECT
    flows[i]["id.orig_h"] AS s_ip;
    flows[i]["id.resp_h"] AS d_ip;
    sum(flows[i]["orig_pkts" | 0]) as orig_pkts;
    sum(flows[i]["orig_bytes" | 0]) as orig_bytes;
    ORDER BY orig_pkts DESC LIMIT 10 FOR r1;  orig_bytes DESC LIMIT 10 FOR r2
update var i set
    r1[i]["server"] as srv_names[r1[i]["src"] | []]

update var i set
    r2[i]["server"] as srv_names[r2[i]["src"] | []]

write write_path/top_bytes_sender.json from r2; 
	  write_path/top_pkts_sender.json from r1

##################
# Beacons - Get connections start with regular intervals
read read_path_flow as flows
create [{"src":s_ip, "dst":d_ip, "intervals": window_ct}] as r0
var i  select
    flows[i]["id.orig_h"] AS s_ip;
    flows[i]["id.resp_h"] AS d_ip;
    int(epoch_time(flows[i]["ts"])/300) as window_start;
    count distinct(window_start) group by s_ip,d_ip as window_ct;
    order by window_ct DESC LIMIT 100
write write_path/top_consistent_interval.json from r

##################
# DNS - As channel
read read_dns_path as flows; 
write_path/ip_to_servers.json as srv_names
create [{"src":s_ip, "base_domain":domain, "requests": ct}] as r
var i  select 
    flows[i]["id.orig_h"] as s_ip;
    ".".join(flows[i]["query"].split(".")[-2:]) as domain;
    count distinct(i) GROUP BY s_ip,domain AS ct;
    order by ct DESC
update r var i set
    r[i]["server"] as srv_names[r[i]["src"] | []]
 write write_path/dns_domain_requests.json from r   

#correlate the top connection/concatenated duration and top bytes sender
read write_path/top_duration.json as top_dur_ip; write_path/top_bytes_sender.json as top_byte_ip
create {s_ip_1: srv} as r
var i, j select
    top_byte_ip[i]["src"] as s_ip_1;
    top_byte_ip[i]["server"] as srv;
    top_dur_ip[j]["src"] as s_ip_2;
where s_ip_1 == s_ip_2
WRITE write_path/top_duration_and_out_byte.json from r